# Dataset

This directory contains the dataset from the MaViLS research project.

**MaViLS** is an acronym for **"Matching Videos to Lecture Slides"**.

## Source

The dataset is obtained directly from the official MaViLS repository without any modifications:
- **Official Repository**: [https://github.com/andererka/MaViLS](https://github.com/andererka/MaViLS)
- **Modifications**: None - the dataset is used as-is from the original source

## About MaViLS

MaViLS is a benchmark dataset for video-to-slide alignment, assessing baseline accuracy with a multimodal alignment algorithm leveraging speech, OCR, and visual features. The dataset includes:

- 20 labeled lectures from various study fields
- Over 22 hours of video content
- 12,830 distinct video segments with accompanying audio transcripts and slide labels
- Ground truth files matching slides to video frames and spoken sentences

**Paper**: [https://www.isca-archive.org/interspeech_2024/anderer24_interspeech.pdf](https://www.isca-archive.org/interspeech_2024/anderer24_interspeech.pdf)

## License

Apache-2.0 License

Please refer to the [original repository](https://github.com/andererka/MaViLS) for detailed licensing information.
